{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "\n",
    "import utils\n",
    "import scoring\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE = True\n",
    "REPEATITION=50\n",
    "\n",
    "\n",
    "log_dir = \"./logs/fit2/\"\n",
    "\n",
    "#output folder for saving checkpoint\n",
    "outputFolder = './output2'\n",
    "if not os.path.exists(outputFolder):\n",
    "    os.makedirs(outputFolder)\n",
    "    \n",
    "%load_ext tensorboard\n",
    "# !rm -rf log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# columns = utils.SIMPLE_FEATURE_COLUMNS + [\"id\", \"label\", \"weight\", \"sWeight\", \"kinWeight\"]\n",
    "# DATA_PATH = \"~/share/data/I-coopetition-muon-id/\"\n",
    "# train = pd.read_csv(os.path.join(DATA_PATH, \"train.csv.gz\"), index_col=\"id\", usecols=columns)\n",
    "# test = pd.read_csv(os.path.join(DATA_PATH, \"test-features.csv.gz\"), index_col=\"id\", usecols=utils.SIMPLE_FEATURE_COLUMNS + [\"id\"])\n",
    "\n",
    "\n",
    "columns = utils.SIMPLE_FEATURE_COLUMNS + [\"id\", \"label\", \"weight\",  \"kinWeight\"]\n",
    "DATA_FOLDER = \"~/share/data/1.6.2-boosting/\"\n",
    "data = pd.read_csv(os.path.join(DATA_FOLDER, \"train_1_percent.csv\"), index_col=\"id\", usecols=columns)\n",
    "\n",
    "\n",
    "new_key_label=[]\n",
    "for name in data.columns:\n",
    "    if '[' in name:\n",
    "        name = name.replace('[', '_').replace(']', '')\n",
    "\n",
    "    new_key_label.append(name)\n",
    "\n",
    "data.columns = new_key_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncl_0</th>\n",
       "      <th>ncl_1</th>\n",
       "      <th>ncl_2</th>\n",
       "      <th>ncl_3</th>\n",
       "      <th>avg_cs_0</th>\n",
       "      <th>avg_cs_1</th>\n",
       "      <th>avg_cs_2</th>\n",
       "      <th>avg_cs_3</th>\n",
       "      <th>ndof</th>\n",
       "      <th>MatchedHit_TYPE_0</th>\n",
       "      <th>...</th>\n",
       "      <th>Mextra_DY2_0</th>\n",
       "      <th>Mextra_DY2_1</th>\n",
       "      <th>Mextra_DY2_2</th>\n",
       "      <th>Mextra_DY2_3</th>\n",
       "      <th>FOI_hits_N</th>\n",
       "      <th>PT</th>\n",
       "      <th>P</th>\n",
       "      <th>label</th>\n",
       "      <th>kinWeight</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156079</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>1.153846</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>609.451230</td>\n",
       "      <td>1758.819800</td>\n",
       "      <td>4016.10500</td>\n",
       "      <td>7918.94200</td>\n",
       "      <td>7</td>\n",
       "      <td>1699.454257</td>\n",
       "      <td>8692.546221</td>\n",
       "      <td>1</td>\n",
       "      <td>2.80650</td>\n",
       "      <td>2.431642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129839</th>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.769231</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>342.040680</td>\n",
       "      <td>989.623600</td>\n",
       "      <td>2251.83450</td>\n",
       "      <td>4411.09700</td>\n",
       "      <td>5</td>\n",
       "      <td>3381.439699</td>\n",
       "      <td>13378.001174</td>\n",
       "      <td>1</td>\n",
       "      <td>1.85646</td>\n",
       "      <td>2.259009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263309</th>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>2.296296</td>\n",
       "      <td>1.384615</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>256.180200</td>\n",
       "      <td>744.244750</td>\n",
       "      <td>1684.75510</td>\n",
       "      <td>3292.70750</td>\n",
       "      <td>12</td>\n",
       "      <td>962.785246</td>\n",
       "      <td>13904.582780</td>\n",
       "      <td>1</td>\n",
       "      <td>3.77936</td>\n",
       "      <td>4.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199338</th>\n",
       "      <td>130</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>3.423077</td>\n",
       "      <td>3.045454</td>\n",
       "      <td>1.434783</td>\n",
       "      <td>1.565217</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>649.849800</td>\n",
       "      <td>1885.884300</td>\n",
       "      <td>4325.38430</td>\n",
       "      <td>8559.56000</td>\n",
       "      <td>64</td>\n",
       "      <td>810.190921</td>\n",
       "      <td>8565.752002</td>\n",
       "      <td>1</td>\n",
       "      <td>1.85646</td>\n",
       "      <td>-0.740073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2663246</th>\n",
       "      <td>85</td>\n",
       "      <td>61</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>3.294118</td>\n",
       "      <td>2.278688</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.617317</td>\n",
       "      <td>36.086983</td>\n",
       "      <td>79.09466</td>\n",
       "      <td>148.79881</td>\n",
       "      <td>10</td>\n",
       "      <td>1452.814360</td>\n",
       "      <td>64736.044173</td>\n",
       "      <td>1</td>\n",
       "      <td>1.85646</td>\n",
       "      <td>1.161770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ncl_0  ncl_1  ncl_2  ncl_3  avg_cs_0  avg_cs_1  avg_cs_2  avg_cs_3  \\\n",
       "id                                                                            \n",
       "156079      24      2     11     13  2.333333  2.000000  1.727273  1.153846   \n",
       "1129839     24     13      6     10  1.708333  1.769231  1.500000  1.400000   \n",
       "4263309     27     13     10     13  2.296296  1.384615  1.100000  2.000000   \n",
       "199338     130     66     23     23  3.423077  3.045454  1.434783  1.565217   \n",
       "2663246     85     61     18     20  3.294118  2.278688  1.222222  1.300000   \n",
       "\n",
       "         ndof  MatchedHit_TYPE_0  ...  Mextra_DY2_0  Mextra_DY2_1  \\\n",
       "id                                ...                               \n",
       "156079      8                  2  ...    609.451230   1758.819800   \n",
       "1129839     8                  2  ...    342.040680    989.623600   \n",
       "4263309     8                  2  ...    256.180200    744.244750   \n",
       "199338      8                  2  ...    649.849800   1885.884300   \n",
       "2663246     8                  2  ...     12.617317     36.086983   \n",
       "\n",
       "         Mextra_DY2_2  Mextra_DY2_3  FOI_hits_N           PT             P  \\\n",
       "id                                                                           \n",
       "156079     4016.10500    7918.94200           7  1699.454257   8692.546221   \n",
       "1129839    2251.83450    4411.09700           5  3381.439699  13378.001174   \n",
       "4263309    1684.75510    3292.70750          12   962.785246  13904.582780   \n",
       "199338     4325.38430    8559.56000          64   810.190921   8565.752002   \n",
       "2663246      79.09466     148.79881          10  1452.814360  64736.044173   \n",
       "\n",
       "         label  kinWeight    weight  \n",
       "id                                   \n",
       "156079       1    2.80650  2.431642  \n",
       "1129839      1    1.85646  2.259009  \n",
       "4263309      1    3.77936  4.349500  \n",
       "199338       1    1.85646 -0.740073  \n",
       "2663246      1    1.85646  1.161770  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (8087,)\n",
      "Validation labels shape: (2022,)\n",
      "Test labels shape: (2528,)\n",
      "Training features shape: (8087, 67)\n",
      "Validation features shape: (2022, 67)\n",
      "Test features shape: (2528, 67)\n"
     ]
    }
   ],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# train_ds = utils.df_to_dataset(train_df, shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "# val_ds = utils.df_to_dataset(val_df, shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "# test_ds = utils.df_to_dataset(test_df, shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('label'))\n",
    "val_labels = np.array(val_df.pop('label'))\n",
    "test_labels = np.array(test_df.pop('label'))\n",
    "# val_weights = np.array(val_df.copy('weight'))\n",
    "# val_weights = val_df['weight'].copy()\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)\n",
    "\n",
    "\n",
    "train_ds = utils.make_ds(train_features, train_labels , shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "val_ds = utils.make_ds(val_features, val_labels,  shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "# test_ds = utils.make_ds(test_features, test_labels, shuffle=SHUFFLE, batch_size=BATCH_SIZE, repeatitions = REPEATITION)\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels))#.cache()\n",
    "# val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "\n",
    "# for feature_batch, label_batch in train_ds.take(1):\n",
    "#   print('Every feature:', list(feature_batch.keys()))\n",
    "#   print('A batch of PT:', feature_batch['PT'])\n",
    "#   print('A batch of targets:', label_batch )\n",
    "\n",
    "# feature_columns = []\n",
    "# feature_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# for header in list(feature_batch.keys()):\n",
    "#     feature_columns.append(tf.feature_column.numeric_column(header))\n",
    "\n",
    "\n",
    "# # # print(feature_columns)\n",
    "# feature_layer = tf.keras.layers.DenseFeatures(feature_columns, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found latest checkpoint----:  None\n",
      "found model checkpoint-----: ./output2/model-106_0.8672.ckpt\n",
      ".....loading model from checkpoint:  ./output2/model-106_0.8672.ckpt\n"
     ]
    }
   ],
   "source": [
    "METRICS = [\n",
    "#       keras.metrics.TruePositives(name='tp'),\n",
    "#       keras.metrics.FalsePositives(name='fp'),\n",
    "#       keras.metrics.TrueNegatives(name='tn'),\n",
    "#       keras.metrics.FalseNegatives(name='fn'),\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "def make_model(metrics = METRICS, output_bias=None):\n",
    "\n",
    "  model = keras.Sequential([\n",
    "#       keras.layers.Dense(1280, activation='relu', input_shape=(train_features.shape[-1],), name=\"layer1\" ),\n",
    "#       feature_layer,\n",
    "      keras.layers.Dense(1280, activation='relu', name=\"layer1\" ),\n",
    "      keras.layers.Dense(640, activation='relu', name=\"layer2\"),\n",
    "      keras.layers.Dropout(.1, name=\"dropout1\"),\n",
    "      keras.layers.Dense(320, activation='relu', name=\"layer3\"),\n",
    "      keras.layers.Dropout(.05, name=\"dropout2\"),\n",
    "      keras.layers.Dense(160, activation='relu', name=\"layer4\"),\n",
    "      keras.layers.Dropout(.025, name=\"dropout3\"),\n",
    "      keras.layers.Dense(80, activation='relu', name=\"layer5\"),\n",
    "      keras.layers.Dropout(.0125, name=\"dropout4\"),\n",
    "      keras.layers.Dense(40, activation='relu', name=\"layer6\"),\n",
    "      keras.layers.Dense(20, activation='relu', name=\"layer7\"),\n",
    "      keras.layers.Dense(1, name=\"layer8\"),\n",
    "      keras.layers.Dense(1, activation='sigmoid',\n",
    "                         bias_initializer=output_bias),\n",
    "  ])\n",
    "\n",
    "#   model.compile(optimizer='rmsprop',\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#               metrics=metrics)\n",
    "    \n",
    "  model.compile(\n",
    "#       optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "      optimizer='rmsprop',\n",
    "      loss=keras.losses.BinaryCrossentropy(),\n",
    "      metrics=metrics)\n",
    "\n",
    "#   model.compile(optimizer='adam',\n",
    "#                 loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                 metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "RESTORE = True\n",
    "latest_ckpt = tf.train.latest_checkpoint(outputFolder)\n",
    "print('found latest checkpoint----: ', latest_ckpt)\n",
    "\n",
    "latest_model_ckpt = utils.latest_saved_model(outputFolder)\n",
    "\n",
    "if latest_model_ckpt is not None and RESTORE:\n",
    "    print('.....loading model from checkpoint: ', latest_model_ckpt)\n",
    "    model = tf.keras.models.load_model(latest_model_ckpt)\n",
    "    model_history = model.history\n",
    "elif latest_ckpt is not None and RESTORE:\n",
    "    model = make_model()\n",
    "    model.load_weights(latest_ckpt).assert_consumed()\n",
    "    print(\"Restored from {}\".format(latest_ckpt))\n",
    "    model_history = model.history\n",
    "else:\n",
    "    model = make_model()\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "# model.save_weights(checkpoint_path.format(epoch=0, val_auc=0))\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    verbose=1,\n",
    "    patience=20,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = outputFolder+\"/model-{epoch:02d}_{val_auc:.4f}.ckpt\"\n",
    "# /model-{epoch:02d}-{val_auc:.2f}.ckpt\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_auc',\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1,\n",
    "                                                 mode='max',\n",
    "                                                 save_best_only=True)\n",
    "\n",
    "cp_callback_weightOnly = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 monitor='val_auc',\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1,\n",
    "                                                 mode='max',\n",
    "                                                 save_best_only=True)\n",
    "\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard( log_dir=log_dir + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                                histogram_freq=1,\n",
    "                                                write_graph=True,\n",
    "                                                write_images=False,\n",
    "                                                update_freq='epoch',\n",
    "                                                profile_batch=2,\n",
    "                                                embeddings_freq=0,\n",
    "                                                embeddings_metadata=None,\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "###TO-DO Write call back to save training history for every epoch\n",
    "#https://github.com/tensorflow/tensorflow/issues/27861\n",
    "#https://stackoverflow.com/questions/50127527/how-to-save-training-history-on-every-epoch-in-keras\n",
    "\n",
    "# if (latest_ckpt is None  and latest_model_ckpt is  None) :\n",
    "# model_history = model.fit(\n",
    "#     train_features,\n",
    "#     train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     steps_per_epoch = 20,\n",
    "#     epochs=EPOCHS,\n",
    "#     shuffle=True,\n",
    "#     callbacks = [early_stopping, cp_callback, tensorboard_callback],\n",
    "#     validation_data=val_ds)\n",
    "        #     validation_data=(val_features, val_labels))\n",
    "\n",
    "if (latest_ckpt is None  and latest_model_ckpt is  None) :\n",
    "    model_history = model.fit(\n",
    "    train_ds,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "    steps_per_epoch = 50,\n",
    "    epochs=EPOCHS,\n",
    "    shuffle=False,\n",
    "    callbacks = [early_stopping, cp_callback, tensorboard_callback],\n",
    "    validation_data=val_ds)\n",
    "\n",
    "# model.fit(train_features,\n",
    "#     train_labels,\n",
    "#           validation_data=val_ds,\n",
    "#           epochs=10)\n",
    "# loss, accuracy = model.evaluate(test_ds)\n",
    "# print(\"Accuracy\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-48ce2232c48a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# json.dumps(str(model.history) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplot_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-48ce2232c48a>\u001b[0m in \u001b[0;36mplot_metrics\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapitalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     plt.plot(history.epoch, history.history['val_'+metric],\n\u001b[1;32m      8\u001b[0m              color=colors[0], linestyle=\"--\", label='Val')\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'epoch'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "execution_count": 87,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'auc', 'precision', 'recall']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.5,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "# with open(outputFolder+\"history.json\", 'w') as fp:\n",
    "# json.dumps(str(model.history) )\n",
    "    \n",
    "plot_metrics(model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
    "val_predictions_baseline = model.predict(val_features, batch_size=BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "  cm = confusion_matrix(labels, predictions > p)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "\n",
    "baseline_results = model.evaluate(test_features, test_labels,\n",
    "                                  batch_size=BATCH_SIZE, verbose=0)\n",
    "for name, value in zip(model.metrics_names, baseline_results):\n",
    "  print(name, ': ', value)\n",
    "print()\n",
    "\n",
    "plot_cm(test_labels, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "#   plt.xlim([-0.5,20])\n",
    "#   plt.ylim([0,100.5])\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color=colors[0])\n",
    "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color=colors[1], linestyle='--')\n",
    "plot_roc(\"Validation Baseline\", val_labels, val_predictions_baseline, color=colors[2], linestyle='-.')\n",
    "\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# print(val_predictions_baseline)\n",
    "# print(val_predictions_baseline.shape)\n",
    "# val_predictions_baseline.flatten()\n",
    "# print(val_predictions_baseline.flatten())\n",
    "# print(val_labels)\n",
    "# # val_weights = np.array(val_df.copy('weight'))\n",
    "# # val_weights = val_df['weight'].copy().values\n",
    "# # print(val_weights)\n",
    "\n",
    "# print(len(val_predictions_baseline.flatten()))\n",
    "# print(len(val_labels))\n",
    "# print(len(val_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "scoring.rejection90(val_labels, val_predictions_baseline.flatten(), sample_weight=val_df['weight'].copy().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}